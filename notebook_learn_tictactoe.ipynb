{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL agent Q-learning for TicTacToe env\n",
    "\n",
    "The [Tic-Tac-Toe](https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment) is a simple game environment that allows to train reinforcement learning agents. These notebook contains an implemetation of Q-learning with epsilon-greedy strategy for TicTacToe env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the python modules\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym_TicTacToe\n",
    "\n",
    "from src.qagent import QLearningAgent\n",
    "from src.play_tictactoe import play_tictactoe, play_tictactoe_with_random\n",
    "\n",
    "from src.utils import (\n",
    "    create_state_dictionary,\n",
    "    reshape_state,\n",
    "    save_qtable,\n",
    "    load_qtable\n",
    ")\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, color, episodes: int):\n",
    "        self.color = color\n",
    "        self.reward_array = np.zeros(episodes)\n",
    "        self.reset_reward()\n",
    "        self.name = f\"Player {color}\"\n",
    "\n",
    "    def reset_reward(self):\n",
    "        self.reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# initialize the tictactoe environment\n",
    "env = gym.envs.make(\"TTT-v0\", small=-1, large=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legal states: 17906\n"
     ]
    }
   ],
   "source": [
    "state_dict = create_state_dictionary()\n",
    "state_size = len(state_dict.keys())\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "episodes = 660_000\n",
    "max_steps = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_parameters = {\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.0,\n",
    "    \"decay_rate\": 0.00001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "qagent = QLearningAgent(exploration_parameters, state_size, action_size, learning_rate=0.1, gamma=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def play(qagent:QLearningAgent, player: Player, state: int, action_space: np.array) -> tuple:\n",
    "    action = qagent.get_action(state, action_space)\n",
    "\n",
    "    # remove action from the action space\n",
    "    action_space = action_space[action_space != action]\n",
    "\n",
    "    new_state, reward, done, _ = env.step((action, player.color))\n",
    "    #TODO: maybe should change a marker after this agent turn \n",
    "    new_state = np.append(new_state, player.color)\n",
    "    new_state = state_dict[reshape_state(new_state)] \n",
    "\n",
    "    qagent.qtable[state, action] = qagent.update_qtable(\n",
    "        state, new_state, action, reward, done\n",
    "    )\n",
    "    # new state\n",
    "    state = new_state\n",
    "    return state, action_space, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random(qagent:QLearningAgent, player: Player, state: int, action_space: np.array) -> tuple:\n",
    "    action = np.random.choice(action_space)\n",
    "    action_space = action_space[action_space != action]\n",
    "    new_state, reward, done, _ = env.step((action, player.color))\n",
    "    new_state = np.append(new_state, player.color)\n",
    "    new_state = state_dict[reshape_state(new_state)]\n",
    "    state = new_state\n",
    "    return state, action_space, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "visited_states = np.zeros((state_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/660000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [14:46<00:00, 744.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.55, Win rate: 0.68, Gamma: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:02<00:00, 731.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.55, Win rate: 0.6, Gamma: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:08<00:00, 726.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.55, Win rate: 0.66, Gamma: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:12<00:00, 723.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.55, Win rate: 0.58, Gamma: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:24<00:00, 713.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.65, Win rate: 0.76, Gamma: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:15<00:00, 720.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.65, Win rate: 0.72, Gamma: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:15<00:00, 720.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.65, Win rate: 0.7, Gamma: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:30<00:00, 709.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.65, Win rate: 0.58, Gamma: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:51<00:00, 693.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.8, Win rate: 0.68, Gamma: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:45<00:00, 697.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.8, Win rate: 0.58, Gamma: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:43<00:00, 699.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.8, Win rate: 0.8, Gamma: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:44<00:00, 698.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.8, Win rate: 0.7, Gamma: 0.99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:50<00:00, 694.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.9, Win rate: 0.66, Gamma: 0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:31<00:00, 708.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.9, Win rate: 0.76, Gamma: 0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:24<00:00, 713.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.9, Win rate: 0.62, Gamma: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 660000/660000 [15:22<00:00, 715.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate: 0.9, Win rate: 0.72, Gamma: 0.99\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "player_1 = Player(color=1, episodes=episodes)\n",
    "player_2 = Player(color=2, episodes=episodes)\n",
    "\n",
    "track_progress = np.zeros(episodes)\n",
    "\n",
    "win_history = []\n",
    "\n",
    "rewards = []\n",
    "lear_rate = [0.55, 0.65, 0.8, 0.9]\n",
    "# lr = 0.4, gamma = 0.9, winrate = 0.64\n",
    "# Learning rate: 0.5, Win rate: 0.72, Gamma: 0.9\n",
    "# Learning rate: 0.6, Win rate: 0.5, Gamma: 0.9\n",
    "# Learning rate: 0.7, Win rate: 0.66, Gamma: 0.9\n",
    "# Learning rate: 0.65, Win rate: 0.76, Gamma: 0.8\n",
    "\n",
    "# best \n",
    "# Learning rate: 0.8, Win rate: 0.8, Gamma: 0.9\n",
    "gamma = [0.8, 0.85, 0.9, 0.99]\n",
    "\n",
    "\n",
    "for lr in lear_rate:\n",
    "    for g in gamma:\n",
    "        qagent = QLearningAgent(exploration_parameters, state_size, action_size, learning_rate=lr, gamma=g)\n",
    "        # qagent_old = qagent\n",
    "        for episode in tqdm(range(episodes)):\n",
    "            \n",
    "            action_space = np.arange(9)\n",
    "\n",
    "            player_1.reset_reward()\n",
    "            player_2.reset_reward()\n",
    "\n",
    "            # randomly change the order players\n",
    "            start = np.random.choice([1,2])\n",
    "\n",
    "            state, _ = env.reset()\n",
    "            state = np.append(state, start)\n",
    "            state = state_dict[reshape_state(state)]\n",
    "\n",
    "            # if episode % 10_000 == 0:\n",
    "                # save_qtable(qagent.qtable, 'tables', \"q_table_old\")\n",
    "\n",
    "            for _step in range(start, max_steps + start):\n",
    "\n",
    "                # change a turn\n",
    "                if _step % 2 == 0:\n",
    "                    #state, action_space, done = play_random(qagent, player_1, state, action_space)\n",
    "                    # qagent_old.qtable = load_qtable('tables', \"q_table_old\")\n",
    "                    state, action_space, done = play(qagent, player_1, state, action_space)\n",
    "                else:\n",
    "                    state, action_space, done = play(qagent, player_2, state, action_space)\n",
    "                visited_states[state] += 1\n",
    "                if done == True:\n",
    "                    break\n",
    "\n",
    "            # reduce epsilon for exporation-exploitation tradeoff\n",
    "            qagent.update_epsilon(episode)\n",
    "\n",
    "        cur_win_rate, reward = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=100)\n",
    "        print(f\"Learning rate: {lr}, Win rate: {sum(cur_win_rate)/100}, Gamma: {g}\")\n",
    "\n",
    "            #check how good is agent\n",
    "            # if episode % 1_000 == 0:\n",
    "            #     num_games = 50\n",
    "            #     cur_win_rate, reward = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=num_games)\n",
    "            #     win_history.append(sum(cur_win_rate)/num_games)\n",
    "            #     print(\"WinRate:\", sum(cur_win_rate)/num_games)\n",
    "            #     # rewards.append(reward)\n",
    "            #     # clear_output(True)\n",
    "            #     # # plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n",
    "            #     # plt.plot(rewards)\n",
    "            #     # plt.show()\n",
    "            # if episode % 25_000 == 0:\n",
    "\n",
    "            #     sum_q_table = np.sum(qagent.qtable)\n",
    "            #     time_passed = round((time.time() - start_time) / 60.0, 2)\n",
    "\n",
    "            #     print(\n",
    "            #         f\"episode: {episode}, \\\n",
    "            #         epsilon: {round(qagent.epsilon, 2)}, \\\n",
    "            #         sum q-table: {sum_q_table}, \\\n",
    "            #         elapsed time [min]: {time_passed},  \\\n",
    "            #         done [%]: {episode / episodes * 100} \\\n",
    "            #         \"\n",
    "            #     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent: 61.17502513124092\n"
     ]
    }
   ],
   "source": [
    "visited_states.shape[0]\n",
    "print(\"Percent:\",100*np.sum(visited_states > 0)/visited_states.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinRate: 0.382\n"
     ]
    }
   ],
   "source": [
    "num_games = 1000\n",
    "cur_win_rate, _ = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=num_games)\n",
    "win_history.append(sum(cur_win_rate)/num_games)\n",
    "print(\"WinRate:\", sum(cur_win_rate)/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table_with_penalty.npy saved!\n"
     ]
    }
   ],
   "source": [
    "qtable = qagent.qtable\n",
    "save_qtable(qtable, 'tables', \"q_table_with_penalty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = load_qtable('tables', \"q_table_067\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5055\n",
      "[[0 0 0]\n",
      " [0 1 0]\n",
      " [0 0 2]]\n",
      "Turn was: 2\n",
      "[[5.1 5.6 6.3]\n",
      " [6.4 0.  6.4]\n",
      " [6.1 6.5 0. ]]\n"
     ]
    }
   ],
   "source": [
    "#check how correct is q-table\n",
    "\n",
    "state = np.random.choice(np.arange(env.observation_space.n))\n",
    "# state_dict[state]\n",
    "print(state)\n",
    "\n",
    "key = list(filter(lambda x: state_dict[x] == state, state_dict))[0]\n",
    "print(np.array(key[:-1]).reshape(3,3))\n",
    "print(\"Turn was:\", key[-1])\n",
    "print(np.round(qagent.qtable[state].reshape(3,3),1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent beginns\n",
      "--------------------\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 7\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ O │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 4\n",
      "-1\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ O │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 0\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ O │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 2\n",
      "-1\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ O │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 6\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ O │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 8\n",
      "-1\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ O │ X │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 5\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ - │ X │ O │\n",
      "├───┼───┼───┤\n",
      "│ O │ O │ X │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 3\n",
      "-1\n",
      "\n",
      "\n",
      "╒═══╤═══╤═══╕\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ X │ X │ O │\n",
      "├───┼───┼───┤\n",
      "│ O │ O │ X │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 1\n",
      "\n",
      "\n",
      "There is no Winner!\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_tictactoe(env, qagent.qtable, state_dict, num_test_games=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6050d35557e2eda2bee3489ac5b9239cf3ea28e67ca6bb3b65a2efaf99506245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tictactoe_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
