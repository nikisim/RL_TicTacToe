{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL agent Q-learning for TicTacToe env\n",
    "\n",
    "The [Tic-Tac-Toe](https://github.com/MauroLuzzatto/OpenAI-Gym-TicTacToe-Environment) is a simple game environment that allows to train reinforcement learning agents. These notebook contains an implemetation of Q-learning with epsilon-greedy strategy for TicTacToe env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the python modules\n",
    "import time\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import gym_TicTacToe\n",
    "\n",
    "from src.qagent import QLearningAgent\n",
    "from src.play_tictactoe import play_tictactoe, play_tictactoe_with_random\n",
    "\n",
    "from src.utils import (\n",
    "    create_state_dictionary,\n",
    "    reshape_state,\n",
    "    save_qtable,\n",
    "    load_qtable\n",
    ")\n",
    "\n",
    "# ignore warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, color, episodes: int):\n",
    "        self.color = color\n",
    "        self.reward_array = np.zeros(episodes)\n",
    "        self.name = f\"Player {color}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the tictactoe environment\n",
    "\n",
    "REWARD_LARGE = 10\n",
    "REWARD_SMALL = -1\n",
    "REWARD_FOR_BLOCK = 8\n",
    "\n",
    "env = gym.envs.make(\"TTT-v0\", small=REWARD_SMALL, large=REWARD_LARGE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of legal states: 12092\n"
     ]
    }
   ],
   "source": [
    "state_dict = create_state_dictionary()\n",
    "state_size = len(state_dict.keys())\n",
    "action_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set training parameters\n",
    "episodes = 960_000\n",
    "max_steps = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "exploration_parameters = {\n",
    "    \"max_epsilon\": 1.0,\n",
    "    \"min_epsilon\": 0.0,\n",
    "    \"decay_rate\": 0.00001,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating QLearning agent with learning params\n",
    "\n",
    "lear_rate = 0.8\n",
    "gamma = 0.9\n",
    "qagent = QLearningAgent(exploration_parameters, state_size, action_size, learning_rate=lear_rate, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_potential_lose(state, color: int) -> bool:\n",
    "        \"\"\"check if after agent's move there is a lose position\n",
    "\n",
    "        Args:\n",
    "            color (int): of the player's enemy\n",
    "\n",
    "        Returns:\n",
    "            bool: indicating if this was a crucial move\n",
    "        \"\"\"\n",
    "        state_check = np.copy(state)\n",
    "        lose = False\n",
    "        col = np.array([1,2])\n",
    "        #enemy color\n",
    "        enemy_color = color\n",
    "        player_color = col[col != enemy_color][0]\n",
    "        state_check[state_check == player_color] = -1\n",
    "        state_check[state_check == enemy_color] = 1\n",
    "        state_check = state_check.reshape(3,3)\n",
    "        for ii in range(3):\n",
    "            if (\n",
    "                # check columns\n",
    "                np.sum(state_check[:, ii]) == 2\n",
    "                # check rows\n",
    "                or np.sum(state_check[ii, :]) == 2\n",
    "                # check diagonal\n",
    "                or np.sum([state_check[0, 0], state_check[1, 1], state_check[2, 2]])\n",
    "                == 2\n",
    "                or np.sum([state_check[0, 2], state_check[1, 1], state_check[2, 0]])\n",
    "                == 2\n",
    "            ):\n",
    "                lose = True\n",
    "                break\n",
    "        return lose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse dict where keys are int\n",
    "reverse_dict = {value: key for key, value in state_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(qagent:QLearningAgent, player_color, state: int, action_space: np.array) -> tuple:\n",
    "    action = qagent.get_action(state, action_space)\n",
    "\n",
    "    pure_state = np.array(reverse_dict[state][:-1])\n",
    "    # remove action from the action space\n",
    "    action_space = action_space[action_space != action]\n",
    "    # potential lose\n",
    "    pot_lose = False\n",
    "    col = np.array([1,2])\n",
    "    if check_for_potential_lose(pure_state, col[col != player_color][0]):\n",
    "        pot_lose = True\n",
    "\n",
    "    new_state, reward, done, _ = env.step((action, player_color))\n",
    "\n",
    "    # check for block opponent's win\n",
    "    # if agent blocks opponnt's win agent get a reward\n",
    "    if (done == False):\n",
    "        if not check_for_potential_lose(new_state, col[col != player_color][0]) and pot_lose == True:\n",
    "            reward += REWARD_FOR_BLOCK\n",
    "\n",
    "    # Add player mark to a state\n",
    "    new_state = np.append(new_state, player_color)\n",
    "    new_state = state_dict[reshape_state(new_state)] \n",
    "\n",
    "    # Q-table update\n",
    "    qagent.qtable[state, action] = qagent.update_qtable(\n",
    "        state, new_state, action, reward, done\n",
    "    )\n",
    "    # new state\n",
    "    state = new_state\n",
    "    return state, action_space, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_random(qagent:QLearningAgent, player_color, state: int, action_space: np.array) -> tuple:\n",
    "    action = np.random.choice(action_space)\n",
    "    action_space = action_space[action_space != action]\n",
    "    new_state, reward, done, _ = env.step((action, player_color))\n",
    "    new_state = np.append(new_state, player_color)\n",
    "    new_state = state_dict[reshape_state(new_state)]\n",
    "    state = new_state\n",
    "    return state, action_space, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 258/960000 [00:00<12:17, 1301.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinRate: 0.84\n",
      "episode: 0,             epsilon: 1.0,             sum q-table: 297841.31641134125,             elapsed time [min]: 0.0,              done [%]: 0.0             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 25228/960000 [00:16<10:14, 1520.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinRate: 0.76\n",
      "episode: 25000,             epsilon: 0.78,             sum q-table: 317420.58225588524,             elapsed time [min]: 0.27,              done [%]: 2.604166666666667             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 50192/960000 [00:32<10:13, 1482.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinRate: 0.76\n",
      "episode: 50000,             epsilon: 0.61,             sum q-table: 321340.6304481548,             elapsed time [min]: 0.54,              done [%]: 5.208333333333334             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 73346/960000 [00:47<09:34, 1544.67it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 38\u001b[0m\n\u001b[1;32m     36\u001b[0m         state, action_space, done \u001b[38;5;241m=\u001b[39m play(qagent, player_1\u001b[38;5;241m.\u001b[39mcolor, state, action_space)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 38\u001b[0m     state, action_space, done \u001b[38;5;241m=\u001b[39m \u001b[43mplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mqagent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_space\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[79], line 13\u001b[0m, in \u001b[0;36mplay\u001b[0;34m(qagent, player_color, state, action_space)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_for_potential_lose(pure_state, col[col \u001b[38;5;241m!=\u001b[39m player_color][\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     11\u001b[0m     pot_lose \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m new_state, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplayer_color\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# check for block opponent's win\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# if agent blocks opponnt's win agent get a reward\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (done \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Courses/OpenAI-Gym-TicTacToe-Environment/gym-TicTacToe/gym_TicTacToe/envs/tictactoe_env.py:73\u001b[0m, in \u001b[0;36mTicTacToeEnv.step\u001b[0;34m(self, user_action)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[row, col] \u001b[38;5;241m=\u001b[39m color  \u001b[38;5;66;03m# postion the token on the field\u001b[39;00m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;66;03m# col = [1,2]\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_is_winner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# done = self._is_winner(col[color != col])\u001b[39;00m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/Courses/OpenAI-Gym-TicTacToe-Environment/gym-TicTacToe/gym_TicTacToe/envs/tictactoe_env.py:99\u001b[0m, in \u001b[0;36mTicTacToeEnv._is_winner\u001b[0;34m(self, color)\u001b[0m\n\u001b[1;32m     92\u001b[0m bool_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m color\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ii \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;66;03m# check if three equal coins are aligned (horizontal, verical or diagonal)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m     96\u001b[0m         \u001b[38;5;66;03m# check columns\u001b[39;00m\n\u001b[1;32m     97\u001b[0m         np\u001b[38;5;241m.\u001b[39msum(bool_matrix[:, ii]) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m     98\u001b[0m         \u001b[38;5;66;03m# check rows\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbool_matrix\u001b[49m\u001b[43m[\u001b[49m\u001b[43mii\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    100\u001b[0m         \u001b[38;5;66;03m# check diagonal\u001b[39;00m\n\u001b[1;32m    101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum([bool_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m], bool_matrix[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], bool_matrix[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m]])\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    103\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum([bool_matrix[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m], bool_matrix[\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], bool_matrix[\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m]])\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    105\u001b[0m     ):\n\u001b[1;32m    106\u001b[0m         done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36msum\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:2324\u001b[0m, in \u001b[0;36msum\u001b[0;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[1;32m   2321\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[1;32m   2322\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[0;32m-> 2324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2325\u001b[0m \u001b[43m                      \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[0;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import random\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "player_1 = Player(color=1, episodes=episodes)\n",
    "player_2 = Player(color=2, episodes=episodes)\n",
    "\n",
    "win_history = []\n",
    "\n",
    "# best learning params\n",
    "# Learning rate: 0.8, Win rate: 0.8, Gamma: 0.9\n",
    "\n",
    "for episode in tqdm(range(episodes)):\n",
    "    action_space = np.arange(9)\n",
    "\n",
    "    # randomly change the order players\n",
    "    start = np.random.choice([1,2])\n",
    "\n",
    "    state, _ = env.reset()\n",
    "    state = np.append(state, start)\n",
    "    state = state_dict[reshape_state(state)]\n",
    "\n",
    "    # One game\n",
    "    for _step in range(start, max_steps + start):\n",
    "        if _step == max_steps + start - 1:\n",
    "            last_turn = True\n",
    "        # change a turn\n",
    "        if _step % 2 == 0:\n",
    "            if episode % 5 == 0:\n",
    "                # Sometimes play against random\n",
    "                state, action_space, done = play_random(qagent, player_1.color, state, action_space)\n",
    "            else:\n",
    "                state, action_space, done = play(qagent, player_1.color, state, action_space)\n",
    "        else:\n",
    "            state, action_space, done = play(qagent, player_2.color, state, action_space)\n",
    "        if done == True:\n",
    "            break\n",
    "\n",
    "    # reduce epsilon for exporation-exploitation tradeoff\n",
    "    qagent.update_epsilon(episode)\n",
    "\n",
    "    #cur_win_rate, reward = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=100)\n",
    "\n",
    "    #check how good is an agent\n",
    "    if episode % 25_000 == 0:\n",
    "        num_games = 50\n",
    "        cur_win_rate, reward = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=num_games)\n",
    "        win_history.append(sum(cur_win_rate)/num_games)\n",
    "        print(\"WinRate:\", sum(cur_win_rate)/num_games)\n",
    "        # rewards.append(reward)\n",
    "        # clear_output(True)\n",
    "        # # plt.title('eps = {:e}, mean reward = {:.1f}'.format(agent.epsilon, np.mean(rewards[-10:])))\n",
    "        # plt.plot(rewards)\n",
    "        # plt.show()\n",
    "    if episode % 25_000 == 0:\n",
    "\n",
    "        sum_q_table = np.sum(qagent.qtable)\n",
    "        time_passed = round((time.time() - start_time) / 60.0, 2)\n",
    "\n",
    "        print(\n",
    "            f\"episode: {episode}, \\\n",
    "            epsilon: {round(qagent.epsilon, 2)}, \\\n",
    "            sum q-table: {sum_q_table}, \\\n",
    "            elapsed time [min]: {time_passed},  \\\n",
    "            done [%]: {episode / episodes * 100} \\\n",
    "            \"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WinRate: 0.9104\n"
     ]
    }
   ],
   "source": [
    "# Win Rate check\n",
    "\n",
    "num_games = 10000\n",
    "cur_win_rate, _ = play_tictactoe_with_random(env, qagent.qtable, state_dict, num_test_games=num_games)\n",
    "win_history.append(sum(cur_win_rate)/num_games)\n",
    "print(\"WinRate:\", sum(cur_win_rate)/num_games)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_table_best_09.npy saved!\n"
     ]
    }
   ],
   "source": [
    "qtable = qagent.qtable\n",
    "save_qtable(qtable, 'tables', \"q_table_best_09\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtable = load_qtable('tables', \"q_table_best2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6479\n",
      "[[1 2 1]\n",
      " [2 0 1]\n",
      " [0 1 2]]\n",
      "Turn was: 1\n",
      "[[ 0.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [-1.  0.  0.]]\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "#check how correct is q-table\n",
    "\n",
    "state = np.random.choice(np.arange(env.observation_space.n))\n",
    "# state_dict[state]\n",
    "print(state)\n",
    "\n",
    "key = list(filter(lambda x: state_dict[x] == state, state_dict))[0]\n",
    "print(np.array(key[:-1]).reshape(3,3))\n",
    "print(\"Turn was:\", key[-1])\n",
    "print(np.round(qagent.qtable[state].reshape(3,3),2))\n",
    "\n",
    "# q = np.round(qtable[state],2)\n",
    "# print(\"Action: \",np.argmax(q))\n",
    "\n",
    "state_pure = np.array(key[:-1])\n",
    "action_space = np.where(state_pure == 0)[0]\n",
    "\n",
    "best_action = max(action_space, key=lambda action: qagent.qtable[state, action])\n",
    "print(best_action)\n",
    "# array = np.array(qtable[state, :])\n",
    "# order = array.argsort()\n",
    "# ranks = order.argsort()\n",
    "# max_value_rank = np.min(ranks[action_space])\n",
    "# action = np.where(ranks == max_value_rank)[0][0]\n",
    "# action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent beginns\n",
      "--------------------\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 1\n",
      "╒═══╤═══╤═══╕\n",
      "│ - │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 0\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "-1\n",
      "\n",
      "\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 6\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 5\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ - │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "-1\n",
      "\n",
      "\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 3\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 4\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ - │\n",
      "╘═══╧═══╧═══╛\n",
      "-1\n",
      "\n",
      "\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 8\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ - │ O │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "--------------------\n",
      "Move Human\n",
      "Action: 7\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ - │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ O │\n",
      "╘═══╧═══╧═══╛\n",
      "-1\n",
      "\n",
      "\n",
      "--------------------\n",
      "move Agent\n",
      "Action: 2\n",
      "╒═══╤═══╤═══╕\n",
      "│ X │ O │ O │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ X │\n",
      "├───┼───┼───┤\n",
      "│ O │ X │ O │\n",
      "╘═══╧═══╧═══╛\n",
      "\n",
      "\n",
      "There is no Winner!\n",
      "--------------------\n",
      "--------------------\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "play_tictactoe(env, qtable, state_dict, num_test_games=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6050d35557e2eda2bee3489ac5b9239cf3ea28e67ca6bb3b65a2efaf99506245"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('tictactoe_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
